{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e6bc301",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.6.0+cu124'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Tuple, Dict, List\n",
    "import random\n",
    "import re\n",
    "\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ba015df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84c402c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/tank/scratch/azaikina/esm/embeds\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "embeddings_path = Path.cwd().parent.parent / \"esm\" / \"embeds\"\n",
    "print(embeddings_path.resolve())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48dd7b9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 0 directories and 16691 images in '/mnt/tank/scratch/azaikina/esm/embeds'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "def walk_through_dir(dir_path):\n",
    "  \"\"\"\n",
    "  Walks through dir_path returning its contents.\n",
    "  Args:\n",
    "    dir_path (str or pathlib.Path): target directory\n",
    "  \n",
    "  Returns:\n",
    "    A print out of:\n",
    "      number of subdiretories in dir_path\n",
    "      number of images (files) in each subdirectory\n",
    "      name of each subdirectory\n",
    "  \"\"\"\n",
    "  for dirpath, dirnames, filenames in os.walk(dir_path):\n",
    "    print(f\"There are {len(dirnames)} directories and {len(filenames)} images in '{dirpath}'.\")\n",
    "\n",
    "walk_through_dir(embeddings_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f770d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numpy_embed_path = list(Path(embeddings_path).glob(\"*.npy\"))[0]\n",
    "numpy_embed = np.load(numpy_embed_path)\n",
    "tensor = torch.from_numpy(numpy_embed).type(torch.float32)\n",
    "tensor.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b421fcc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingsDataset(Dataset):\n",
    "    def __init__(self, embeddings_path: str, with_name=False):\n",
    "        self.paths = list(Path(embeddings_path).glob(\"*.npy\"))\n",
    "        self.with_name = with_name\n",
    "    def load_embedding(self, index: int, dtype = torch.float32) -> torch.Tensor:\n",
    "        \"Transforms numpy to torch.Tensor.\"\n",
    "        npy_path = self.paths[index]\n",
    "        npy_embed = np.load(npy_path)\n",
    "        tensor = torch.from_numpy(npy_embed).type(dtype)\n",
    "        return tensor\n",
    "    def __len__(self) -> int:\n",
    "        \"Returns the total number of embeddings.\"\n",
    "        return len(self.paths)\n",
    "    def __getitem__(self, index: int):\n",
    "        \"Returns one sample of data and original file name.\"\n",
    "        embedding = self.load_embedding(index)\n",
    "        name = str(self.paths[index])\n",
    "\n",
    "        if self.with_name:\n",
    "            return embedding, name # return tensor, file name\n",
    "        else:\n",
    "            return embedding # return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "628d4678",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.EmbeddingsDataset at 0x7f96acc810f0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_custom = EmbeddingsDataset(embeddings_path=embeddings_path, with_name=True)\n",
    "data_custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c3a9440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16690\n",
      "random samples\n",
      "(tensor([ 0.0103, -0.1574, -0.0748,  ..., -0.0995, -0.0991,  0.0440]), '/mnt/tank/scratch/azaikina/esm/embeds/esm_embed_19630.npy')\n",
      "(tensor([ 0.0789, -0.0314, -0.0532,  ..., -0.0026,  0.0069,  0.0122]), '/mnt/tank/scratch/azaikina/esm/embeds/esm_embed_6826.npy')\n",
      "(tensor([ 0.0103, -0.1574, -0.0748,  ..., -0.0995, -0.0991,  0.0440]), '/mnt/tank/scratch/azaikina/esm/embeds/esm_embed_4472.npy')\n",
      "(tensor([ 0.0687, -0.0285, -0.0512,  ..., -0.0046, -0.0021,  0.0105]), '/mnt/tank/scratch/azaikina/esm/embeds/esm_embed_9102.npy')\n",
      "(tensor([ 0.0564, -0.0473, -0.0747,  ..., -0.1364, -0.0472,  0.0989]), '/mnt/tank/scratch/azaikina/esm/embeds/esm_embed_13857.npy')\n",
      "(tensor([ 0.0822, -0.0282, -0.0428,  ..., -0.0012, -0.0089,  0.0079]), '/mnt/tank/scratch/azaikina/esm/embeds/esm_embed_11822.npy')\n",
      "(tensor([ 0.0103, -0.1574, -0.0748,  ..., -0.0995, -0.0991,  0.0440]), '/mnt/tank/scratch/azaikina/esm/embeds/esm_embed_4734.npy')\n",
      "(tensor([ 0.0172,  0.0108, -0.0695,  ..., -0.0100, -0.0675,  0.0116]), '/mnt/tank/scratch/azaikina/esm/embeds/esm_embed_12846.npy')\n"
     ]
    }
   ],
   "source": [
    "print(len(data_custom))\n",
    "random_samples_idx = random.sample(range(len(data_custom)), k=8)\n",
    "print('random samples')\n",
    "\n",
    "for i, targ_sample in enumerate(random_samples_idx):\n",
    "        targ_emb = data_custom[targ_sample]\n",
    "        print(targ_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f05d8fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3214\n"
     ]
    }
   ],
   "source": [
    "text = '/mnt/tank/scratch/azaikina/esm/embeds/esm_embed_3214.npy'\n",
    "\n",
    "text.find('*/*.npy')\n",
    "\n",
    "s = 'esm/embeds/esm_embed_3214.npy'\n",
    "match = re.search(r\"(\\d+)(?=\\.npy$)\", s)\n",
    "if match:\n",
    "    print(match.group(1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f3748c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<torch.utils.data.dataloader.DataLoader at 0x7f96acc82740>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x7f96acc80ee0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataloader_custom = DataLoader(dataset=data_custom,\n",
    "                                     batch_size=5,\n",
    "                                     shuffle=True)\n",
    "\n",
    "test_dataloader_custom = DataLoader(dataset=data_custom,\n",
    "                                    batch_size=1,\n",
    "                                    shuffle=False)\n",
    "\n",
    "train_dataloader_custom, test_dataloader_custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3cca2a8e",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m img_custom \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(train_dataloader_custom))\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Batch size will now be 1, try changing the batch_size parameter above and see what happens\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImage shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mimg_custom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m -> [batch_size, color_channels, height, width]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "# Get image and label from custom DataLoader\n",
    "img_custom = next(iter(train_dataloader_custom))\n",
    "\n",
    "# Batch size will now be 1, try changing the batch_size parameter above and see what happens\n",
    "print(f\"Image shape: {img_custom.shape} -> [batch_size, color_channels, height, width]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ac63cf",
   "metadata": {},
   "source": [
    "# Aptamers dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa3c08ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_path = data_path / '3_checked_intersections.csv'\n",
    "df = pd.read_csv(df_path, index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c22a3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#original class\n",
    "class AptamersDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, embeddings_path: str,\n",
    "                 ab_name_column: str = 'Name of Antibody', apt_name_column: str = 'Name of Aptamer', tg_name_column: str = 'Target_ab',\n",
    "                 ab_seq_column: str = 'Antibody Sequence', apt_seq_column: str = 'Aptamer Sequence', tg_seq_column: str = 'target_seq_ab',\n",
    "                 with_names: bool =True):\n",
    "        self.df = df\n",
    "        self.embeddings_path = embeddings_path\n",
    "\n",
    "        self.ab_name_column = ab_name_column\n",
    "        self.apt_name_column = apt_name_column\n",
    "        self.tg_name_column = tg_name_column\n",
    "        self.ab_seq_column = ab_seq_column\n",
    "        self.apt_seq_column = apt_seq_column\n",
    "        self.tg_seq_column = tg_seq_column\n",
    "\n",
    "        self.with_names = with_names\n",
    "        self.embeddings_path = list(Path(embeddings_path).glob(\"*.npy\"))\n",
    "\n",
    "    def load_embedding(self, index: int, dtype = torch.float32) -> torch.Tensor:\n",
    "        \"Transforms numpy to torch.Tensor.\"\n",
    "        npy_path = self.embeddings_path[index]\n",
    "        npy_embed = np.load(npy_path)\n",
    "        tensor = torch.from_numpy(npy_embed).type(dtype)\n",
    "        return tensor\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"Returns the total number of samples.\"\n",
    "        return len(df)\n",
    "    \n",
    "    def __getitem__(self, index: int):\n",
    "        embedding = self.load_embedding(index)\n",
    "        emb_path = str(self.embeddings_path[index])\n",
    "        match = re.search(r\"(\\d+)(?=\\.npy$)\", emb_path)\n",
    "        if match:\n",
    "            emb_index = match.group(1)   # path to index; '/esm/embeds/esm_embed_3214.npy' -> 3214   \n",
    "\n",
    "        ab_name = df.loc[emb_index, self.ab_name_column]\n",
    "        apt_name = df.loc[emb_index, self.apt_name_column]\n",
    "        tg_name = df.loc[emb_index, self.tg_name_column]\n",
    "        \n",
    "        ab_seq = df.loc[emb_index, self.ab_seq_column]\n",
    "        apt_seq = df.loc[emb_index, self.apt_seq_column]\n",
    "        tg_seq = df.loc[emb_index, self.tg_seq_column]\n",
    "\n",
    "\n",
    "        if self.with_names:\n",
    "            return embedding, ab_name, apt_name, tg_name, ab_seq, apt_seq, tg_seq\n",
    "        else:\n",
    "            return embedding # return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecff538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16693\n",
      "random samples\n",
      "15831 (tensor([ 0.0750, -0.0339, -0.0551,  ..., -0.0058, -0.0065,  0.0141]), '6iuv_H_L_A', 'APIPred_1934', 'hemagglutinin', 'QVQLVQSGAEVKETGESLNISCKVSGNNFPSYYISWVRQMPGNGLEWMGRIDPSDSDTNYRPSFQGHVTISADKSTSTAYLQWRSLKASDTAMYYCARRATYYYGSGSYFDAFDIWGQGTMVTVSSASTKGPSVFPLAPSSKSTSGGTAALGCLVKDYFPEPVTVSWNSGALTSGVHTFPAVLQSSGLYSLSSVVTVPSSSLGTQTYICNVNHKPSNTKVDKKVEPKSC|EIVMTQSPLTLPVTPGAPASISCRSSQSLLHSDGYNYLDWYLQKPGQSPQLLIYLGSHRASGVPDRFSGSGSGTDFTLKISRVEAEDVGVYYCMQALQTPDFGQGTRLEIKRTVAAPSVFIFPPSDEQLKSGTASVVCLLNNFYPREAKVQWKVDNALQSGNSQESVTEQDSKDSTYSLSSTLTLSKADYEKHKVYACEVTHQGLSSPVTKSFNRGEC', 'GAATGAGGAATAATCTAGCTCCTTCGCTGA', 'NGVKPLILRDCSVAGWLLGNPMCDEFINVPEWSYIVEKASPANDLCYPGNFNDYEELKHLLSRINHFEKIQIIPKSSWSNHDASSGVSSACPYLGRSSFFRNVVWLIKKNSAYPTIKRSYNNTNQEDLLVLWGIHHPNDAAEQTKLYQNPTTYISVGTSTLNQRLVPEIATRPKVNGQSGRMEFFWTILKPNDAINFESNGNFIAPEYAYKIVKKGDSTIMKSE')\n",
      "8998 (tensor([ 0.0399, -0.0264, -0.0521,  ..., -0.1692, -0.0243,  0.0457]), '7u9p_H_L_A', 'C9', 'spike glycoprotein', 'VQLLEESGGGAVQPGRSLRLSCEASGFSFNSYGMHWVRQAPGKGLEWVAAIWYSGSDRDYADSVKGRFSISRDNSKNTLYLQMNSLRAEDTAVYYCARDPHCTGGVCDAFDLWGQGTMVTVSSASTKGPSVFPGAPSSKSTSGGTAALGCLVKDYFPEPVTVSWNSGALTSGVHTFPAVLQSSGLYSLSSVVTVPSSSLGTQTYICNVDHKPATPRWTRKLSPNLVTKL|ELQMTQSPSSLSASVGDRVTITCRASQSISSYLNWYQQKPGKAPKLLIYAASSLQSGVPSRFSGSGSGTDFTLTISSLQPEDFATYYCQQSYSTPYTFGQGTKLEIKRTVAAPSVFIFPPSDEQLKSGTASVVCLLNNFYPREAKVQWKVDNALQSGNSQESVTEQDSKDSTYSLSSTLTLSKADYEKHKVYACEVTHQGLSSPVTKSFNRGEC', \"5'CACGACGACAGAGACCACAGGGGGGCGTCAAGCGGGGTCACATCGGAGTAGGGAATCTTGTGTTCGTCTCTGGCTGCTGG3'\", 'QCVNLTTRTQLPPAYTNSFTRGVYYPDKVFRSSVLHSTQDLFLPFFSNVTWFHAIHVSGTNGTKRFDNPVLPFNDGVYFASTEKSNIIRGWIFGTTLDSKTQSLLIVNNATNVVIKVCEFQFCNDPFLGVYYHKNNKSWMESEFRVYSSANNCTFEYVSQPFLMDLEGKQGNFKNLREFVFKNIDGYFKIYSKHTPINLVRDLPQGFSALEPLVDLPIGINITRFQTLLALHRSYLTPGDSSSGWTAGAAAYYVGYLQPRTFLLKYNENGTITDAVDCALDPLSETKCTLKSFTVEKGIYQTSNFRVQPTESIVRFPNITNLCPFGEVFNATRFASVYAWNRKRISNCVADYSVLYNSASFSTFKCYGVSPTKLNDLCFTNVYADSFVIRGDEVRQIAPGQTGKIADYNYKLPDDFTGCVIAWNSNNLDSKVGGNYNYLYRLFRKSNLKPFERDISTEIYQAGSTPCNGVEGFNCYFPLQSYGFQPTNGVGYQPYRVVVLSFELLHAPATVCGPKKSTNLVKNKCVNFNFNGLTGTGVLTESNKKFLPFQQFGRDIADTTDAVRDPQTLEILDITPCSFGGVSVITPGTNTSNQVAVLYQDVNCTEVPVAIHADQLTPTWRVYSTGSNVFQTRAGCLIGAEHVNNSYECDIPIGAGICASYQTQTNSPGSASSVASQSIIAYTMSLGAENSVAYSNNSIAIPTNFTISVTTEILPVSMTKTSVDCTMYICGDSTECSNLLLQYGSFCTQLNRALTGIAVEQDKNTQEVFAQVKQIYKTPPIKDFGGFNFSQILPDPSKPSKRSPIEDLLFNKVTLADAGFIKQYGDCLGDIAARDLICAQKFNGLTVLPPLLTDEMIAQYTSALLAGTITSGWTFGAGPALQIPFPMQMAYRFNGIGVTQNVLYENQKLIANQFNSAIGKIQDSLSSTPSALGKLQDVVNQNAQALNTLVKQLSSNFGAISSVLNDILSRLDPPEAEVQIDRLITGRLQSLQTYVTQQLIRAAEIRASANLAATKMSECVLGQSKRVDFCGKGYHLMSFPQSAPHGVVFLHVTYVPAQEKNFTTAPAICHDGKAHFPREGVFVSNGTHWFVTQRNFYEPQIITTDNTFVSGNCDVVIGIVNNTVYDPLQPELDSFKEELDKYFKNHTSPDVDLGDISGINASVVNIQKEIDRLNEVAKNLNESLIDLQELGKYEQGSGYIPEAPRDGQAYVRKDGEWVLLSTFLGRSLEVLFQGPGHHHHHHHHSAWSHPQFEKGGGSGGGGSGGSAWSHPQFEK')\n",
      "624 (tensor([ 0.0704, -0.0663, -0.0493,  ..., -0.1888, -0.0922,  0.0199]), '5fv2_B_V', 'APIPred_2109', 'vascular endothelial growth factor', 'EVQLLVSGGGLVQPGGSLRLSCAASGFTFKAYPMMWVRQAPGKGLEWVSEISPSGSYTYYADSVKGRFTISRDNSKNTLYLQMNSLRAEDTAVYYCAKDPRKLDYWGQGTLVTVSS|', 'GGGAGACAAGAATAAACGCTCAACAGAGAAGAAGCACCGATGCAACGCGAAAAGCACACGGCCTTCGACAGGAGGCTCACAACAGGC', 'APMAEGGGQNHHEVVKFMDVYQRSYCHPIETLVDIFQEYPDEIEYIFKPSCVPLMRCGGCCNDEGLECVPTEESNITMQIMRIKPHQGQHIGEMSFLQHNKCECRPKKDRHHHHHH')\n",
      "1677 (tensor([-0.0259, -0.0906, -0.0262,  ..., -0.2077,  0.1201,  0.1566]), '1tzh_B_A_W', 'APIPred_2248', 'vascular endothelial growth factor a', 'EVQLVESGGGLVQPGGSLRLSCAASGFDIYDDDIHWVRQAPGKGLEWVAYIAPSYGYTDYADSVKGRFTISADTSKNTAYLQMNSLRAEDTAVYYCSRSSDASYSYSAMDYWGQGTLVTVSSASTKGPSVFPLAPSSKSTSGGTAALGCLVKDYFPEPVTVSWNSGALTSGVHTFPAVLQSSGLYSLSSVVTVPSSSLGTQTYICNVNHKPSNTKVDKKVEPKSCDKTH|DIQMTQSPSSLSASVGDRVTITCRASQASYSSVAWYQQKPGKAPKLLIYAASYLYSGVPSRFSGSGSGTDFTLTISSLQPEDFATYYCQSSASPATFGQGTKVEIKRTVAAPSVFIFPPSDEQLKSGTASVVCLLNNFYPREAKVQWKVDNALQSGNSQESVTEQDSKDSTYSLSSTLTLSKADYEKHKVYACEVTHQGLSSPVTKSFNRGEC', 'UCAAUCGGCGCUUUACUCUUGCGCUCACCGUGCCC', 'GQNHHEVVKFMDVYQRSYCHPIETLVDIFQEYPDEIEYIFKPSCVPLMRCGGCCNDEGLECVPTEESNITMQIMRIKPHQGQHIGEMSFLQHNKCECRPKKD')\n",
      "16380 (tensor([ 0.0176, -0.0115, -0.0419,  ..., -0.1782, -0.0065,  0.0196]), '7tgf_B_A', 'APIPred_1172', 'ricin chain a', 'QVQLAETGGGLVQPGGSLRLSCAASGFTLDDYAIGWFRQAPGKEREGVSCISSSDGRTYYADSVKGRFTISRDNAKNTVYLQMNSLKSEDTAVYYCATEEVCTLGIFGHGPDDYWGQGTQVTVSSEPKTPKPQ|', 'TAATACGACTCACTATAGGGAGACCCAAGCCGATTTATTTTGTGCAGCTTTTGTTCCCTTTAGTGAGGGTTAATT', 'IFPKQYPIINFTTAGATVQSYTNFIRAVRGRLTTGADVRHEIPVLPNRVGLPINQRFILVELSNHAELSVTLALDVTNAYVVGYRAGNSAYFFHPDNQEDAEAITHLFTDVQNRYTFAFGGNYDRLEQLAGNLRENIELGNGPLEEAISALYYYSTGGTQLPTLARSFIICIQMISEAARFQYIEGEMRTRIRYNRRSAPDPSVITLENSWGRLSTAIQESNQGAFASPIQLQRRNGSKFSVYDVSILIPIIALMVYRCAPPPSSQF')\n",
      "1009 (tensor([ 0.0140,  0.0051, -0.0782,  ..., -0.0086, -0.0643,  0.0191]), '3etb_G_g_K', 'APIPred_554', 'anthrax protective antigen', 'MADYKDIQMTQTTSSLSASLGDRVTVSCRASQDIRNYLNWYQQKPDGTVKFLIYYTSRLQPGVPSRFSGSGSGTDYSLTINNLEQEDIGTYFCQQGNTPPWTFGGGTKLEIKRGGGGSGGGGSGGGGSGGGGSEVQLQQSGPELVKPGASVKISCKDSGYAFNSSWMNWVKQRPGQGLEWIGRIYPGDGDSNYNGKFEGKAILTADKSSSTAYMQLSSLTSVDSAVYFCARSGLLRYAMDYWGQGTSVTVSS|', 'ACCGAAAAAGACCUGACUUCUAUACUAAGUCUACGUUCC', 'RDKRFHYDRNNIAVGADESVVKEAHREVINSSTEGLLLNIDKDIRKILSGYIVEIEDTEGLKEVINDRYDMLNISSLRQDGKTFIDFKKYNDKLPLYISNPNYKVNVYAVTKENTIINPSENGDTSTNGIKKILIFSKKGYEIG')\n"
     ]
    }
   ],
   "source": [
    "#modification of AptamerDataset, \n",
    "#!! need to check if the returned tensor is the right one\n",
    "class AptamersDataset(Dataset):\n",
    "    \"\"\"    \n",
    "    Returns:\n",
    "    A tuple out of:\n",
    "        embedding (type torch.Tensor), \n",
    "        ab_name, apt_name, tg_name, \n",
    "        ab_seq, apt_seq, tg_seq\n",
    "    \"\"\"\n",
    "    def __init__(self, df: pd.DataFrame, embeddings_path: str,\n",
    "                 ab_name_column: str = 'Name of Antibody', apt_name_column: str = 'Name of Aptamer', tg_name_column: str = 'Target_ab',\n",
    "                 ab_seq_column: str = 'Antibody Sequence', apt_seq_column: str = 'Aptamer Sequence', tg_seq_column: str = 'target_seq_ab',\n",
    "                 with_names: bool =True):\n",
    "        self.df = df\n",
    "        self.embeddings_path = embeddings_path\n",
    "\n",
    "        self.ab_name_column = ab_name_column\n",
    "        self.apt_name_column = apt_name_column\n",
    "        self.tg_name_column = tg_name_column\n",
    "        self.ab_seq_column = ab_seq_column\n",
    "        self.apt_seq_column = apt_seq_column\n",
    "        self.tg_seq_column = tg_seq_column\n",
    "\n",
    "        self.with_names = with_names\n",
    "        self.embeddings_path = list(Path(embeddings_path).glob(\"*.npy\"))\n",
    "\n",
    "    def load_embedding(self, index: int, dtype = torch.float32) -> torch.Tensor:\n",
    "        \"Transforms numpy to torch.Tensor.\"\n",
    "        npy_path = self.embeddings_path[index]\n",
    "        npy_embed = np.load(npy_path)\n",
    "        tensor = torch.from_numpy(npy_embed).type(dtype)\n",
    "        return tensor\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"Returns the total number of samples.\"\n",
    "        return len(df)\n",
    "    \n",
    "    def __getitem__(self, index: int):\n",
    "        embedding = self.load_embedding(index)\n",
    "        # emb_path = str(self.embeddings_path[index])\n",
    "        # match = re.search(r\"(\\d+)(?=\\.npy$)\", emb_path)\n",
    "        # if match:\n",
    "        #     emb_index = match.group(1)   # path to index; '/esm/embeds/esm_embed_3214.npy' -> 3214   \n",
    "        emb_index = index\n",
    "        ab_name = df.loc[emb_index, self.ab_name_column]\n",
    "        apt_name = df.loc[emb_index, self.apt_name_column]\n",
    "        tg_name = df.loc[emb_index, self.tg_name_column]\n",
    "        \n",
    "        ab_seq = df.loc[emb_index, self.ab_seq_column]\n",
    "        apt_seq = df.loc[emb_index, self.apt_seq_column]\n",
    "        tg_seq = df.loc[emb_index, self.tg_seq_column]\n",
    "\n",
    "\n",
    "        if self.with_names:\n",
    "            return embedding, ab_name, apt_name, tg_name, ab_seq, apt_seq, tg_seq\n",
    "        else:\n",
    "            return embedding # return tensor\n",
    "\n",
    "\n",
    "apt_dataset = AptamersDataset(df=df, embeddings_path = embeddings_path)\n",
    "len(apt_dataset)\n",
    "\n",
    "print(len(apt_dataset))\n",
    "samples_idx = [15831, 8998, 624, 1677, 16380, 1009]\n",
    "print('random samples')\n",
    "\n",
    "for id in samples_idx:\n",
    "    print(id, apt_dataset[id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6302853",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set embeddings and sequences paths \n",
    "data_path = Path(\"../data/\")\n",
    "#embeddings_path = Path(\"../../esm/embeds\")\n",
    "embeddings_path = Path.cwd().parent.parent / \"esm\" / \"embeds\"  #/mnt/tank/scratch/azaikina/esm/embeds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c5a2f797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting data_setup.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile data_setup.py\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Tuple, Dict, List\n",
    "import random\n",
    "import re\n",
    "\n",
    "#set embeddings and sequences paths \n",
    "data_path = Path(\"../data/\")\n",
    "#embeddings_path = Path(\"../../esm/embeds\")\n",
    "embeddings_path = Path.cwd().parent.parent / \"esm\" / \"embeds\"  #/mnt/tank/scratch/azaikina/esm/embeds\n",
    "\n",
    "df_path = data_path / '3_checked_intersections.csv'\n",
    "df = pd.read_csv(df_path, index_col = 0)\n",
    "\n",
    "class AptamersDataset(Dataset):\n",
    "    \"\"\"    \n",
    "    Returns:\n",
    "    A tuple out of:\n",
    "        embedding (type torch.Tensor), \n",
    "        ab_name, apt_name, tg_name, \n",
    "        ab_seq, apt_seq, tg_seq\n",
    "    \"\"\"\n",
    "    def __init__(self, df: pd.DataFrame, embeddings_path: str,\n",
    "                 ab_name_column: str = 'Name of Antibody', apt_name_column: str = 'Name of Aptamer', tg_name_column: str = 'Target_ab',\n",
    "                 ab_seq_column: str = 'Antibody Sequence', apt_seq_column: str = 'Aptamer Sequence', tg_seq_column: str = 'target_seq_ab',\n",
    "                 with_names: bool =True):\n",
    "        self.df = df\n",
    "        self.embeddings_path = embeddings_path\n",
    "\n",
    "        self.ab_name_column = ab_name_column\n",
    "        self.apt_name_column = apt_name_column\n",
    "        self.tg_name_column = tg_name_column\n",
    "        self.ab_seq_column = ab_seq_column\n",
    "        self.apt_seq_column = apt_seq_column\n",
    "        self.tg_seq_column = tg_seq_column\n",
    "        self.with_names = with_names\n",
    "\n",
    "        #Mapping {embedding_number: file_path}\n",
    "        all_files = list(Path(embeddings_path).glob(\"*.npy\"))\n",
    "        self.id_to_path = {}\n",
    "        for f in all_files:\n",
    "            match = re.search(r\"(\\d+)(?=\\.npy$)\", f.name)\n",
    "            if match:\n",
    "                emb_id = int(match.group(1))\n",
    "                self.id_to_path[emb_id] = f\n",
    "\n",
    "        # --- Keep only DataFrame rows that have an embedding ---\n",
    "        self.valid_indices = [i for i in self.df.index if i in self.id_to_path]\n",
    "        if len(self.valid_indices) < len(self.df):\n",
    "            print(f\"‚ö†Ô∏è {len(self.df) - len(self.valid_indices)} rows skipped (no embedding file found).\")\n",
    "\n",
    "    def load_embedding(self, emb_index: int, dtype = torch.float32) -> torch.Tensor:\n",
    "        \"Transforms numpy to torch.Tensor.\"\n",
    "        npy_path = self.id_to_path[emb_index]\n",
    "        \n",
    "        npy_embed = np.load(npy_path)\n",
    "        tensor = torch.from_numpy(npy_embed).type(dtype)\n",
    "        return tensor\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"Returns the total number of samples.\"\n",
    "        return len(self.valid_indices)\n",
    "    \n",
    "    def __getitem__(self, index: int):\n",
    "        emb_index = self.valid_indices[index]\n",
    "        embedding = self.load_embedding(emb_index)\n",
    "        embedding_path = self.id_to_path[emb_index]\n",
    "\n",
    "\n",
    "        ab_name = self.df.loc[emb_index, self.ab_name_column]\n",
    "        apt_name = self.df.loc[emb_index, self.apt_name_column]\n",
    "        tg_name = self.df.loc[emb_index, self.tg_name_column]\n",
    "        \n",
    "        ab_seq = self.df.loc[emb_index, self.ab_seq_column]\n",
    "        apt_seq = self.df.loc[emb_index, self.apt_seq_column]\n",
    "        tg_seq = self.df.loc[emb_index, self.tg_seq_column]\n",
    "\n",
    "        if self.with_names:\n",
    "            return embedding, embedding_path, ab_name, apt_name, tg_name, ab_seq, apt_seq, tg_seq\n",
    "        else:\n",
    "            return embedding # return tensor\n",
    "    \n",
    "    def get_by_id(self, emb_id: int):\n",
    "        \"\"\"Fetch a sample by its true embedding ID (DataFrame index)\"\"\"\n",
    "        if emb_id not in self.id_to_path:\n",
    "            raise KeyError(f\"No embedding found for ID {emb_id}\")\n",
    "\n",
    "        embedding = self.load_embedding(emb_id)\n",
    "        embedding_path = self.id_to_path[emb_id]\n",
    "\n",
    "\n",
    "        ab_name = self.df.loc[emb_id, self.ab_name_column]\n",
    "        apt_name = self.df.loc[emb_id, self.apt_name_column]\n",
    "        tg_name = self.df.loc[emb_id, self.tg_name_column]\n",
    "        \n",
    "        ab_seq = self.df.loc[emb_id, self.ab_seq_column]\n",
    "        apt_seq = self.df.loc[emb_id, self.apt_seq_column]\n",
    "        tg_seq = self.df.loc[emb_id, self.tg_seq_column]\n",
    "\n",
    "        if self.with_names:\n",
    "            return embedding, embedding_path, ab_name, apt_name, tg_name, ab_seq, apt_seq, tg_seq\n",
    "        else:\n",
    "            return embedding\n",
    "        \n",
    "def collate_embeddings(batch):\n",
    "    \"\"\"\n",
    "    DataLoader cannot batch Path objects or strings by default. \n",
    "    Collate function to batch embeddings (torch.Tensor) and keep other fields as lists.\n",
    "    \"\"\"\n",
    "    embeddings = torch.stack([item[0] for item in batch])  # batch tensor\n",
    "    paths = [item[1] for item in batch]                   # list of Paths\n",
    "    ab_names = [item[2] for item in batch]\n",
    "    apt_names = [item[3] for item in batch]\n",
    "    tg_names = [item[4] for item in batch]\n",
    "    ab_seqs = [item[5] for item in batch]\n",
    "    apt_seqs = [item[6] for item in batch]\n",
    "    tg_seqs = [item[7] for item in batch]\n",
    "\n",
    "    return embeddings, paths, ab_names, apt_names, tg_names, ab_seqs, apt_seqs, tg_seqs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "093f4828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è 4 rows skipped (no embedding file found).\n",
      "16689\n",
      "random samples\n",
      "4 (tensor([ 0.0511, -0.0160, -0.0115,  ..., -0.0082, -0.0421,  0.0080]), PosixPath('/mnt/tank/scratch/azaikina/esm/embeds/esm_embed_4.npy'), '2nz9_D_C_A', 'aptamer_limited_ds_482', 'botulinum neurotoxin type a', 'QVQLQESGGGLVQPGGSLRLSCAASGFTFSDHYMYWVRQAPGKGLEWVATISDGGSYTYYSDSVEGRFTTSRDNSKNTLYLQMNSLRAEDTAIYYCSRYRYDDAMDYWGQGTLVTVSSASTKGPSVFPLAPSSKSTSGGTAALGCLVKDYFPEPVTVSWNSGALTSGVHTFPAVLQSSGLYSLSSVVTVPSSSLGTQTYICNVNHKPSNTKVDKKVEPKSCDKT|EIVLTQSPATLSLSPGERATISCRASESVDSYGHSFMQWYQQKPGQAPRLLIYRASNLEPGIPARFSGSGSGTDFTLTISSLEPEDFAVYYCQQGNEVPFTFGQGTKVEIKRTVAAPSVFIFPPSDEQLKSGTASVVCLLNNFYPREAKVQWKVDNALQSGNSQESVTEQDSKDSTYSLSSTLTLSKADYEKHKVYACEVTHQGLSSPVTKSFNRGEC', 'ATACCAGCTTATTCAATTGACATGACTGGGATTTTTGGCGAAATCGAAGGAAGCGGAGAGATAGTAAGTGCAATCT', 'PFVNKQFNYKDPVNGVDIAYIKIPNVGQMQPVKAFKIHNKIWVIPERDTFTNPEEGDLNPPPEAKQVPVSYYDSTYLSTDNEKDNYLKGVTKLFERIYSTDLGRMLLTSIVRGIPFWGGSTIDTELKVIDTNCINVIQPDGSYRSEELNLVIIGPSADIIQFECKSFGHEVLNLTRNGYGSTQYIRFSPDFTFGFEESLEVDTNPLLGAGKFATDPAVTLAHELIHAGHRLYGIAINPNRVFKVNTNAYYEMSGLEVSFEELRTFGGHDAKFIDSLQENEFRLYYYNKFKDIASTLNKAKSIVGTTASLQYMKNVFKEKYLLSEDTSGKFSVDKLKFDKLYKMLTEIYTEDNFVKFFKVLNRKTYLNFDKAVFKINIVPKVNYTIYDGFNLRNTNLAANFNGQNTEINNMNFTKLKNFTGLFEFYKLLCVRGIITSKTKSLDKGYNKALNDLCIKVNNWDLFFSPSEDNFTNDLNKGEEITSDTNIEAAEENISLDLIQQYYLTFNFDNEPENISIENLSSDIIGQLELMPNIERFPNGKKYELDKYTMFHYLRAQEFEHGKSRIALTNSVNEALLNPSRVYTFFSSDYVKKVNKATEAAMFLGWVEQLVYDFTDETSEVSTTDKIADITIIIPYIGPALNIGNMLYKDDFVGALIFSGAVILLEFIPEIAIPVLGTFALVSYIANKVLTVQTIDNALSKRNEKWDEVYKYIVTNWLAKVNTQIDLIRKKMKEALENQAEATKAIINYQYNQYTEEEKNNINFNIDDLSSKLNESINKAMININKFLNQCSVSYLMNSMIPYGVKRLEDFDASLKDALLKYIYDNRGTLIGQVDRLKDKVNNTLSTDIPFQLSKYVDNQRLLSTFTEYIKNIINTSILNLRYESNHLIDLSRYASKINIGSKVNFDPIDKNQIQLFNLESSKIEVILKNAIVYNSMYENFSTSFWIRIPKYFNSISLNNEYTIINCMENNSGWKVSLNYGEIIWTLQDTQEIKQRVVFKYSQMINISDYINRWIFVTITNNRLNNSKIYINGRLIDQKPISNLGNIHASNNIMFKLDGCRDTHRYIWIKYFNLFDKELNEKEIKDLYDNQSNSGILKDFWGDYLQYDKPYYMLNLYDPNKYVDVNNVGIRGYMYLKGPRGSVMTTNIYLNSSLYRGTKFIIKKYASGNKDNIVRNNDRVYINVVVKNKEYRLATNASQAGVEKILSALEIPDVGNLSQVVVMKSKNDQGITNKCKMNLQDNNGNDIGFIGFHQFNNIAKLVASNWYNRQIERSSRTLGCSWEFIPVDDGWGERPL')\n",
      "1009 (tensor([ 0.0572, -0.0718, -0.0457,  ..., -0.1947, -0.0906,  0.0224]), PosixPath('/mnt/tank/scratch/azaikina/esm/embeds/esm_embed_1161.npy'), '2qr0_L_K_I', 'APIPred_1872', 'vascular endothelial growth factor a', 'EVQLVESGGGLVQPGGSLRLSCAASGFNFSSSSIHWVRQAPGKGLEWVAYIYPSYSYTSYADSVKGRFTISADTSKNTAYLQMNSLRAEDTAVYYCARYYGTGAMDYWGQGTLVTVSSASTKGPSVFPLAPSSKSTSGGTAALGCLVKDYFPEPVTVSWNSGALTSGVHTFPAVLQSSGLYSLSSVVTVPSSSLGTQTYICNVNHKPSNTKVDKKVEPKSC|DIQMTQSPSSLSASVGDRVTITCRASQSVSSAVAWYQQKPGKAPKLLIYSASSLYSGVPSRFSGSRSGTDFTLTISSLQPEDFATYYCQQYSYYYYPFTFGQGTKVEIKRTVAAPSVFIFPPSDEQLKSGTASVVCLLNNFYPREAKVQWKVDNALQSGNSQESVTEQDSKDSTYSLSSTLTLSKADYEKHKVYACEVTHQGLSSPVTKSFNR', 'GAAUCAUACACAAGUUGUGGAG', 'EVVKFMDVYQRSYCHPIETLVDIFQEYPDEIEYIFKPSCVPLMRCGGCCNDEGLECVPTEESNITMQIMRIKPHQGQHIGEMSFLQHNKCECRPKKD')\n"
     ]
    }
   ],
   "source": [
    "#the last modification of AptamerDataset, \n",
    "class AptamersDataset(Dataset):\n",
    "    \"\"\"    \n",
    "    Returns:\n",
    "    A tuple out of:\n",
    "        embedding (type torch.Tensor), \n",
    "        ab_name, apt_name, tg_name, \n",
    "        ab_seq, apt_seq, tg_seq\n",
    "    \"\"\"\n",
    "    def __init__(self, df: pd.DataFrame, embeddings_path: str,\n",
    "                 ab_name_column: str = 'Name of Antibody', apt_name_column: str = 'Name of Aptamer', tg_name_column: str = 'Target_ab',\n",
    "                 ab_seq_column: str = 'Antibody Sequence', apt_seq_column: str = 'Aptamer Sequence', tg_seq_column: str = 'target_seq_ab',\n",
    "                 with_names: bool =True):\n",
    "        self.df = df\n",
    "        self.embeddings_path = embeddings_path\n",
    "\n",
    "        self.ab_name_column = ab_name_column\n",
    "        self.apt_name_column = apt_name_column\n",
    "        self.tg_name_column = tg_name_column\n",
    "        self.ab_seq_column = ab_seq_column\n",
    "        self.apt_seq_column = apt_seq_column\n",
    "        self.tg_seq_column = tg_seq_column\n",
    "        self.with_names = with_names\n",
    "\n",
    "        #Mapping {embedding_number: file_path}\n",
    "        all_files = list(Path(embeddings_path).glob(\"*.npy\"))\n",
    "        self.id_to_path = {}\n",
    "        for f in all_files:\n",
    "            match = re.search(r\"(\\d+)(?=\\.npy$)\", f.name)\n",
    "            if match:\n",
    "                emb_id = int(match.group(1))\n",
    "                self.id_to_path[emb_id] = f\n",
    "\n",
    "        # --- Keep only DataFrame rows that have an embedding ---\n",
    "        self.valid_indices = [i for i in self.df.index if i in self.id_to_path]\n",
    "        if len(self.valid_indices) < len(self.df):\n",
    "            print(f\"‚ö†Ô∏è {len(self.df) - len(self.valid_indices)} rows skipped (no embedding file found).\")\n",
    "\n",
    "    def load_embedding(self, emb_index: int, dtype = torch.float32) -> torch.Tensor:\n",
    "        \"Transforms numpy to torch.Tensor.\"\n",
    "        npy_path = self.id_to_path[emb_index]\n",
    "        \n",
    "        npy_embed = np.load(npy_path)\n",
    "        tensor = torch.from_numpy(npy_embed).type(dtype)\n",
    "        return tensor\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"Returns the total number of samples.\"\n",
    "        return len(self.valid_indices)\n",
    "    \n",
    "    def __getitem__(self, index: int):\n",
    "        emb_index = self.valid_indices[index]\n",
    "        embedding = self.load_embedding(emb_index)\n",
    "        embedding_path = self.id_to_path[emb_index]\n",
    "\n",
    "\n",
    "        ab_name = self.df.loc[emb_index, self.ab_name_column]\n",
    "        apt_name = self.df.loc[emb_index, self.apt_name_column]\n",
    "        tg_name = self.df.loc[emb_index, self.tg_name_column]\n",
    "        \n",
    "        ab_seq = self.df.loc[emb_index, self.ab_seq_column]\n",
    "        apt_seq = self.df.loc[emb_index, self.apt_seq_column]\n",
    "        tg_seq = self.df.loc[emb_index, self.tg_seq_column]\n",
    "\n",
    "        if self.with_names:\n",
    "            return embedding, embedding_path, ab_name, apt_name, tg_name, ab_seq, apt_seq, tg_seq\n",
    "        else:\n",
    "            return embedding # return tensor\n",
    "    \n",
    "    def get_by_id(self, emb_id: int):\n",
    "        \"\"\"Fetch a sample by its true embedding ID (DataFrame index)\"\"\"\n",
    "        if emb_id not in self.id_to_path:\n",
    "            raise KeyError(f\"No embedding found for ID {emb_id}\")\n",
    "\n",
    "        embedding = self.load_embedding(emb_id)\n",
    "        embedding_path = self.id_to_path[emb_id]\n",
    "\n",
    "\n",
    "        ab_name = self.df.loc[emb_id, self.ab_name_column]\n",
    "        apt_name = self.df.loc[emb_id, self.apt_name_column]\n",
    "        tg_name = self.df.loc[emb_id, self.tg_name_column]\n",
    "        \n",
    "        ab_seq = self.df.loc[emb_id, self.ab_seq_column]\n",
    "        apt_seq = self.df.loc[emb_id, self.apt_seq_column]\n",
    "        tg_seq = self.df.loc[emb_id, self.tg_seq_column]\n",
    "\n",
    "        if self.with_names:\n",
    "            return embedding, embedding_path, ab_name, apt_name, tg_name, ab_seq, apt_seq, tg_seq\n",
    "        else:\n",
    "            return embedding\n",
    "        \n",
    "def collate_embeddings(batch):\n",
    "    \"\"\"\n",
    "    DataLoader cannot batch Path objects or strings by default. \n",
    "    Collate function to batch embeddings (torch.Tensor) and keep other fields as lists.\n",
    "    \"\"\"\n",
    "    embeddings = torch.stack([item[0] for item in batch])  # batch tensor\n",
    "    paths = [item[1] for item in batch]                   # list of Paths\n",
    "    ab_names = [item[2] for item in batch]\n",
    "    apt_names = [item[3] for item in batch]\n",
    "    tg_names = [item[4] for item in batch]\n",
    "    ab_seqs = [item[5] for item in batch]\n",
    "    apt_seqs = [item[6] for item in batch]\n",
    "    tg_seqs = [item[7] for item in batch]\n",
    "\n",
    "    return embeddings, paths, ab_names, apt_names, tg_names, ab_seqs, apt_seqs, tg_seqs\n",
    "\n",
    "apt_dataset = AptamersDataset(df=df, embeddings_path = embeddings_path)\n",
    "len(apt_dataset)\n",
    "\n",
    "print(len(apt_dataset))\n",
    "samples_idx = [4, 1009]\n",
    "print('random samples')\n",
    "\n",
    "for id in samples_idx:\n",
    "    print(id, apt_dataset[id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5e2b57ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "1161",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "cf1793ff-6d6a-48e3-b47b-fc47024f17ce",
       "rows": [
        [
         "Target_ab",
         "vascular endothelial growth factor a"
        ],
        [
         "Target_apt",
         "Vascular_endothelial_growth_factor"
        ],
        [
         "Name of Aptamer",
         "APIPred_1872"
        ],
        [
         "Name of Antibody",
         "2qr0_L_K_I"
        ],
        [
         "Aptamer Sequence",
         "GAAUCAUACACAAGUUGUGGAG"
        ],
        [
         "Antibody Sequence",
         "EVQLVESGGGLVQPGGSLRLSCAASGFNFSSSSIHWVRQAPGKGLEWVAYIYPSYSYTSYADSVKGRFTISADTSKNTAYLQMNSLRAEDTAVYYCARYYGTGAMDYWGQGTLVTVSSASTKGPSVFPLAPSSKSTSGGTAALGCLVKDYFPEPVTVSWNSGALTSGVHTFPAVLQSSGLYSLSSVVTVPSSSLGTQTYICNVNHKPSNTKVDKKVEPKSC|DIQMTQSPSSLSASVGDRVTITCRASQSVSSAVAWYQQKPGKAPKLLIYSASSLYSGVPSRFSGSRSGTDFTLTISSLQPEDFATYYCQQYSYYYYPFTFGQGTKVEIKRTVAAPSVFIFPPSDEQLKSGTASVVCLLNNFYPREAKVQWKVDNALQSGNSQESVTEQDSKDSTYSLSSTLTLSKADYEKHKVYACEVTHQGLSSPVTKSFNR"
        ],
        [
         "target_seq_ab",
         "EVVKFMDVYQRSYCHPIETLVDIFQEYPDEIEYIFKPSCVPLMRCGGCCNDEGLECVPTEESNITMQIMRIKPHQGQHIGEMSFLQHNKCECRPKKD"
        ],
        [
         "target_seq_apt",
         "MNFLLSWVHWSLALLLYLHHAKWSQAAPMAEGGGQNHHEVVKFMDVYQRSYCHPIETLVDIFQEYPDEIEYIFKPSCVPLMRCGGCCNDEGLECVPTEESNITMQIMRIKPHQGQHIGEMSFLQHNKCECRPKKDRARQEKKSVRGKGKGQKRKRKKSRYKSWSVYVGARCCLMPWSLPGPHPCGPCSERRKHLFVQDPQTCKCSCKNTDSRCKARQLELNERTCRCDKPRR"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 8
       }
      },
      "text/plain": [
       "Target_ab                         vascular endothelial growth factor a\n",
       "Target_apt                          Vascular_endothelial_growth_factor\n",
       "Name of Aptamer                                           APIPred_1872\n",
       "Name of Antibody                                            2qr0_L_K_I\n",
       "Aptamer Sequence                                GAAUCAUACACAAGUUGUGGAG\n",
       "Antibody Sequence    EVQLVESGGGLVQPGGSLRLSCAASGFNFSSSSIHWVRQAPGKGLE...\n",
       "target_seq_ab        EVVKFMDVYQRSYCHPIETLVDIFQEYPDEIEYIFKPSCVPLMRCG...\n",
       "target_seq_apt       MNFLLSWVHWSLALLLYLHHAKWSQAAPMAEGGGQNHHEVVKFMDV...\n",
       "Name: 1161, dtype: object"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[1161]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b1faa2d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.0572, -0.0718, -0.0457,  ..., -0.1947, -0.0906,  0.0224]),\n",
       " PosixPath('/mnt/tank/scratch/azaikina/esm/embeds/esm_embed_1161.npy'),\n",
       " '2qr0_L_K_I',\n",
       " 'APIPred_1872',\n",
       " 'vascular endothelial growth factor a',\n",
       " 'EVQLVESGGGLVQPGGSLRLSCAASGFNFSSSSIHWVRQAPGKGLEWVAYIYPSYSYTSYADSVKGRFTISADTSKNTAYLQMNSLRAEDTAVYYCARYYGTGAMDYWGQGTLVTVSSASTKGPSVFPLAPSSKSTSGGTAALGCLVKDYFPEPVTVSWNSGALTSGVHTFPAVLQSSGLYSLSSVVTVPSSSLGTQTYICNVNHKPSNTKVDKKVEPKSC|DIQMTQSPSSLSASVGDRVTITCRASQSVSSAVAWYQQKPGKAPKLLIYSASSLYSGVPSRFSGSRSGTDFTLTISSLQPEDFATYYCQQYSYYYYPFTFGQGTKVEIKRTVAAPSVFIFPPSDEQLKSGTASVVCLLNNFYPREAKVQWKVDNALQSGNSQESVTEQDSKDSTYSLSSTLTLSKADYEKHKVYACEVTHQGLSSPVTKSFNR',\n",
       " 'GAAUCAUACACAAGUUGUGGAG',\n",
       " 'EVVKFMDVYQRSYCHPIETLVDIFQEYPDEIEYIFKPSCVPLMRCGGCCNDEGLECVPTEESNITMQIMRIKPHQGQHIGEMSFLQHNKCECRPKKD')"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apt_dataset.get_by_id(1161)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1d651cd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<torch.utils.data.dataloader.DataLoader at 0x7f55462f1c00>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x7f55462f19c0>)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataloader_custom = DataLoader(dataset=apt_dataset,\n",
    "                                     batch_size=5,\n",
    "                                     shuffle=True,\n",
    "                                     collate_fn=collate_embeddings)\n",
    "\n",
    "test_dataloader_custom = DataLoader(dataset=apt_dataset,\n",
    "                                    batch_size=1,\n",
    "                                    shuffle=False,\n",
    "                                    collate_fn=collate_embeddings)\n",
    "\n",
    "train_dataloader_custom, test_dataloader_custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4786817e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: torch.Size([5, 1280])\n",
      "First embedding path: /mnt/tank/scratch/azaikina/esm/embeds/esm_embed_1596.npy\n",
      "First antibody name: 2qr0_H_G_J\n",
      "First aptamer name: APIPred_865\n",
      "First target name: vascular endothelial growth factor a\n",
      "First antibody sequence: EVQLVESGGGLVQPGGSLRLSCAASGFNFSSSSIHWVRQAPGKGLEWVAYIYPSYSYTSYADSVKGRFTISADTSKNTAYLQMNSLRAEDTAVYYCARYYGTGAMDYWGQGTLVTVSSASTKGPSVFPLAPSSKSTSGGTAALGCLVKDYFPEPVTVSWNSGALTSGVHTFPAVLQSSGLYSLSSVVTVPSSSLGTQTYICNVNHKPSNTKVDKKVEPKSC|DIQMTQSPSSLSASVGDRVTITCRASQSVSSAVAWYQQKPGKAPKLLIYSASSLYSGVPSRFSGSRSGTDFTLTISSLQPEDFATYYCQQYSYYYYPFTFGQGTKVEIKRTVAAPSVFIFPPSDEQLKSGTASVVCLLNNFYPREAKVQWKVDNALQSGNSQESVTEQDSKDSTYSLSSTLTLSKADYEKHKVYACEVTHQGLSSPVTKSFNR\n",
      "First aptamer sequence: AGCAGCACAGAGGTCAGATGCGCGCAGAATTTTGAGTCATGTACTAAGGAATTGATTGGTCCTATGCGTGCTACCGTGAA\n",
      "First target sequence: EVVKFMDVYQRSYCHPIETLVDIFQEYPDEIEYIFKPSCVPLMRCGGCCNDEGLECVPTEESNITMQIMRIKPHQGQHIGEMSFLQHNKCECRPKKD\n",
      "Embeddings shape: torch.Size([1, 1280])\n",
      "First embedding path: /mnt/tank/scratch/azaikina/esm/embeds/esm_embed_0.npy\n",
      "First antibody name: 6w41_H_L_C\n",
      "First aptamer name: Aptagen_8041\n",
      "First target name: spike glycoprotein receptor binding domain\n",
      "First antibody sequence: QMQLVQSGTEVKKPGESLKISCKGSGYGFITYWIGWVRQMPGKGLEWMGIIYPGDSETRYSPSFQGQVTISADKSINTAYLQWSSLKASDTAIYYCAGGSGISTPMDVWGQGTTVTVSSASTKGPSVFPLAPSSKSTSGGTAALGCLVKDYFPEPVTVSWNSGALTSGVHTFPAVLQSSGLYSLSSVVTVPSSSLGTQTYICNVNHKPSNTKVDKKVEPKSC|DIQLTQSPDSLAVSLGERATINCKSSQSVLYSSINKNYLAWYQQKPGQPPKLLIYWASTRESGVPDRFSGSGSGTDFTLTISSLQAEDVAVYYCQQYYSTPYTFGQGTKVEIKRTVAAPSVFIFPPSDEQLKSGTASVVCLLNNFYPREAKVQWKVDNALQSGNSQESVTEQDSKDSTYSLSSTLTLSKADYEKHKVYACEVTHQGLSSPVTKSFNRGECS\n",
      "First aptamer sequence: TGTCCATTAACGCCC\n",
      "First target sequence: RVQPTESIVRFPNITNLCPFGEVFNATRFASVYAWNRKRISNCVADYSVLYNSASFSTFKCYGVSPTKLNDLCFTNVYADSFVIRGDEVRQIAPGQTGKIADYNYKLPDDFTGCVIAWNSNNLDSKVGGNYNYLYRLFRKSNLKPFERDISTEIYQAGSTPCNGVEGFNCYFPLQSYGFQPTNGVGYQPYRVVVLSFELLHAPATVCGPKKSTNLVKNKCVNFSGHHHHHH\n"
     ]
    }
   ],
   "source": [
    "# --- Test one batch ---\n",
    "batch = next(iter(train_dataloader_custom))\n",
    "\n",
    "embeddings, paths, ab_names, apt_names, tg_names, ab_seqs, apt_seqs, tg_seqs = batch\n",
    "\n",
    "print(f\"Embeddings shape: {embeddings.shape}\")  # torch.Tensor shape [batch_size, embedding_dim]\n",
    "print(f\"First embedding path: {paths[0]}\")\n",
    "print(f\"First antibody name: {ab_names[0]}\")\n",
    "print(f\"First aptamer name: {apt_names[0]}\")\n",
    "print(f\"First target name: {tg_names[0]}\")\n",
    "print(f\"First antibody sequence: {ab_seqs[0]}\")\n",
    "print(f\"First aptamer sequence: {apt_seqs[0]}\")\n",
    "print(f\"First target sequence: {tg_seqs[0]}\")\n",
    "\n",
    "# --- Test one batch ---\n",
    "batch = next(iter(test_dataloader_custom))\n",
    "\n",
    "embeddings, paths, ab_names, apt_names, tg_names, ab_seqs, apt_seqs, tg_seqs = batch\n",
    "\n",
    "print(f\"Embeddings shape: {embeddings.shape}\")  # torch.Tensor shape [batch_size, embedding_dim]\n",
    "print(f\"First embedding path: {paths[0]}\")\n",
    "print(f\"First antibody name: {ab_names[0]}\")\n",
    "print(f\"First aptamer name: {apt_names[0]}\")\n",
    "print(f\"First target name: {tg_names[0]}\")\n",
    "print(f\"First antibody sequence: {ab_seqs[0]}\")\n",
    "print(f\"First aptamer sequence: {apt_seqs[0]}\")\n",
    "print(f\"First target sequence: {tg_seqs[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2af769",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "üìò Explanation ‚Äî How this dataset is formed\n",
    "\n",
    "Dataset name: AptamersDataset\n",
    "Purpose: Used to feed paired biological sequence data (antibody, aptamer, and target sequences) with their precomputed embeddings into PyTorch models.\n",
    "\n",
    "üìò Structure\n",
    "\n",
    "Each dataset entry corresponds to one record in your DataFrame (df) that has an existing .npy embedding file.\n",
    "\n",
    "Each .npy file (e.g. esm_embed_16380.npy) stores a precomputed embedding vector ‚Äî usually produced by a protein language model such as ESM, ProtBert, or ProtT5.\n",
    "The numeric part of the filename (16380) matches the DataFrame index.\n",
    "\n",
    "üìò Returned tuple\n",
    "\n",
    "Each call to __getitem__(i) returns:\n",
    "\n",
    "Element\tType\tDescription\n",
    "embedding\ttorch.Tensor\tLoaded ESM embedding vector\n",
    "embedding_path\tpathlib.Path\tPath to .npy file\n",
    "ab_name\tstr\tAntibody name\n",
    "apt_name\tstr\tAptamer name\n",
    "tg_name\tstr\tTarget name\n",
    "ab_seq\tstr\tAntibody sequence\n",
    "apt_seq\tstr\tAptamer sequence\n",
    "tg_seq\tstr\tTarget sequence\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "1Ô∏è‚É£__getitem__(self, index)\n",
    "\n",
    "Used internally by PyTorch DataLoader.\n",
    "\n",
    "index is a 0-based position in self.valid_indices, not the actual embedding ID.\n",
    "\n",
    "Converts the DataLoader‚Äôs position index into the corresponding embedding ID:\n",
    "\n",
    "2Ô∏è‚É£ get_by_id(self, emb_id)\n",
    "\n",
    "Fetches a sample by its true embedding ID / DataFrame index, regardless of position in the dataset.\n",
    "\n",
    "Looks up the embedding and the row directly:\n",
    "\n",
    "üìò Example usage\n",
    "apt_dataset = AptamersDataset(df=df, embeddings_path=\"/path/to/embeds\")\n",
    "print(len(apt_dataset))  # number of samples with embeddings\n",
    "\n",
    "# Access a sample by its DataFrame index\n",
    "embedding, path, ab, apt, tg, ab_seq, apt_seq, tg_seq = apt_dataset[0]\n",
    "print(path, embedding.shape)\n",
    "\n",
    "üìò Embedding details\n",
    "\n",
    "    Format: .npy files containing numpy arrays (float32 or float64).\n",
    "\n",
    "    Source: Precomputed embeddings (e.g. from ESM) for each antibody‚Äìaptamer‚Äìtarget triplet.\n",
    "\n",
    "    Shape: Depends on the embedding model (e.g. [1280] for ESM-2, [1024] for ProtT5).\n",
    "\n",
    "    Conversion: Loaded with np.load ‚Üí torch.from_numpy ‚Üí torch.float32.\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "'''\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
