experiment_name: 'Transformer_kld_finetuned_2'
model_basename: 'Transformer_kld_finetuned_2'
batch_size: 16
num_epochs: 100
save_every: 10
dropout: 0.1
lr: 1e-9  #Если новый датасет маленький, можно уменьшить lr (например, 1e-5) — это поможет не разрушить старые веса.
seq_len: 182     #187                                 #max_len + 2 for SOS and EOS tokens
num_heads: 8
num_layers: 2
d_model: 1280
d_ff: 256
latent_dim: 128
input_dim: 2304
beta: 1000
model_folder: 'checkpoints'
preload: True
encoder_hidden_dim: 512
kmer: 1
comments: 'Transformer trained with mirna, finetuned with aptamers, changed lr'
patience: 20                                      #EarlyStopping
delta_for_early_stop: 0.01                        #EarlyStopping
