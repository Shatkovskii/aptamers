experiment_name: 'Transformer_with_mirna_encode'
model_basename: 'Transformer_with_mirna_encode'
batch_size: 128
num_epochs: 150
save_every: 50
dropout: 0.1
lr: 1e-4
seq_len: 182                                      #max_len + 2 for SOS and EOS tokens
num_heads: 8
num_layers: 2
d_model: 1280
d_ff: 256
input_dim: 2304
model_folder: 'checkpoints'
preload: True
encoder_hidden_dim: 512
kmer: 1
comments: 'Transformer_with_mirna, with encoder'
patience: 20                                      #EarlyStopping
delta_for_early_stop: 0.001                        #EarlyStopping
